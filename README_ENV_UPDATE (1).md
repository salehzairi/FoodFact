# ğŸš€ Mise Ã  jour rapide de lâ€™environnement ETL
**(Airflow + MinIO + Spark + DuckDB)**

Salut ğŸ‘‹  
Jâ€™ai mis Ã  jour les fichiers Docker avec toutes les derniÃ¨res corrections.  
ğŸ‘‰ Voici les Ã©tapes Ã  suivre pour avoir **le mÃªme environnement local que moi** :

---

## ğŸ§© Ã‰tapes Ã  suivre

### 1ï¸âƒ£ Fermez vos conteneurs actuels
```bash
docker compose down
```

---

### 2ï¸âƒ£ Remplacez les fichiers
Copiez mes nouvelles versions de :
- `docker-compose.yml`  
- `Makefile`  
- `containers/airflow/Dockerfile`  

(collez-les dans vos dossiers locaux du projet **openfoodfacts-mig8110**)

---

### 3ï¸âƒ£ Placez les **DAGs** et les **scripts ETL**
> Ces rÃ©pertoires sont montÃ©s dans les conteneurs par `docker-compose.yml`.

- **DAGs Airflow** â†’ placez-les dans **`./dags/`**
  - `./dags/bootstrap_minio.py`
  - `./dags/etl_foodfacts_basic.py`

- **Scripts ETL** â†’ placez-les dans **`./jobs/`**
  - `./jobs/extract.py`
  - `./jobs/transform_spark.py`
  - `./jobs/load_duckdb.py`

- (Les autres dossiers montÃ©s par dÃ©faut)  
  - `./data/` â†’ entrepÃ´t DuckDB et donnÃ©es locales  
  - `./logs/` â†’ logs Airflow

> Si les dossiers nâ€™existent pas localement, crÃ©ez-lesÂ : `mkdir -p dags jobs data logs`

---

### 4ï¸âƒ£ Nettoyez les anciens conteneurs et images
```bash
docker system prune -af
```

---

### 5ï¸âƒ£ Rebuild complet
```bash
make up
```
> Equivalent Ã Â : `docker compose build && docker compose up airflow-init && docker compose up -d`

---

### 6ï¸âƒ£ VÃ©rifiez que tout fonctionne
- ğŸŒ¬ï¸ **Airflow** â†’ <http://localhost:8080>  
- ğŸª£ **MinIO** â†’ <http://localhost:9001>

Dans Airflow, vous devez voir les DAGsÂ : `bootstrap_minio` et `etl_foodfacts_basic`.

---

## ğŸ’¡ En cas de problÃ¨me
Si vous avez encore des erreurs :

```bash
docker compose down -v
make up
```

---

âœ… **RÃ©sultat attendu :**  
Vous aurez **exactement le mÃªme environnement que moi**, avec Spark, DuckDB et MinIO configurÃ©s et fonctionnels.
